{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install datasets matplotlib numpy pandas seaborn scikit-learn torch tqdm transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_metric\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label_to_int(intent: str) -> int:\n",
    "    if intent == 'AddToPlaylist':\n",
    "        return 0\n",
    "    elif intent == 'BookRestaurant':\n",
    "        return 1\n",
    "    elif intent == 'GetWeather':\n",
    "        return 2\n",
    "    elif intent == 'PlayMusic':\n",
    "        return 3\n",
    "    elif intent == 'RateBook':\n",
    "        return 4\n",
    "    elif intent == 'SearchCreativeWork':\n",
    "        return 5\n",
    "    elif intent == 'SearchScreeningEvent':\n",
    "        return 6\n",
    "    return -1\n",
    "\n",
    "\n",
    "def adjust_labels(entries: dict) -> dict:\n",
    "    return {\n",
    "        'text': entries['text'],\n",
    "        'intent': [label_to_int(entry) for entry in entries['intent']]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples: dict) -> dict:\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dataset dict with train as dataset.csv and validation as validation_dataset.csv\n",
    "dataset_dict: datasets.DatasetDict = datasets.load_dataset(\n",
    "    'csv',\n",
    "    data_files={\n",
    "        'train': 'data/dataset.csv',\n",
    "        'validation': 'data/validation_dataset.csv'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for split in ['train', 'validation']:\n",
    "    dataset_dict[split] = dataset_dict[split].map(\n",
    "        lambda e: adjust_labels(e), batched=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=True)\n",
    "tokenized_datasets: datasets.DatasetDict = dataset_dict.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"intent\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataloader: DataLoader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(tokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      2\u001b[0m eval_dataloader: Data \u001b[38;5;241m=\u001b[39m DataLoader(tokenized_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataloader: DataLoader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=8)\n",
    "eval_dataloader: DataLoader = DataLoader(tokenized_datasets[\"validation\"], batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This model is equal to BERT + a linear layer for classification. In our custom model we designed a FastText + a hidden layer and linear layer for classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"prajjwal1/bert-tiny\", num_labels=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs: int = 3\n",
    "num_training_steps: int = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "model.eval()\n",
    "preds, trues = [], []\n",
    "for i, batch in tqdm(enumerate(eval_dataloader), desc=\"evaluating\",\n",
    "                     total=eval_dataloader.__len__()):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    _, tag_seq = torch.max(logits, 1)\n",
    "    preds.extend(tag_seq.cpu().detach().tolist())\n",
    "    trues.extend(batch['labels'].cpu().detach().tolist())\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = ['AddToPlaylist', 'BookRestaurant', 'GetWeather', 'PlayMusic', 'RateBook',\n",
    "         'SearchCreativeWork', 'SearchScreeningEvent']\n",
    "print(classification_report(\n",
    "    np.array(trues).flatten(), np.array(preds).flatten(), target_names=names)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(np.array(trues).flatten(), np.array(preds).flatten())\n",
    "df_cm = pd.DataFrame(cm, index=names, columns=names)\n",
    "# config plot sizes\n",
    "sn.set(font_scale=1)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 8}, cmap='coolwarm', linewidth=0.5,\n",
    "           fmt=\"\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
